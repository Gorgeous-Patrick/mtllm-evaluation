{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Structured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = 'results'\n",
    "EVAL_CONFIG = 'eval.config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(EVAL_CONFIG) as f:\n",
    "    eval_config = json.load(f)\n",
    "\n",
    "problem_set = {}\n",
    "for difficulty in eval_config.keys():\n",
    "    if difficulty not in problem_set:\n",
    "        problem_set[difficulty] = []\n",
    "    for problem in eval_config[difficulty].keys():\n",
    "        problem_set[difficulty].append(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EASY': ['translation',\n",
       "  'essay_reviewer',\n",
       "  'joke_gen',\n",
       "  'expert_answer',\n",
       "  'odd_word_out'],\n",
       " 'MEDIUM': ['mcq_reason', 'personality_finder', 'template', 'text_to_type'],\n",
       " 'HARD': ['rpg_level_gen', 'wikipedia']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_DIFFICULTY = 'EASY' # 'EASY', 'MEDIUM', 'HARD'\n",
    "\n",
    "def get_results(difficulty):\n",
    "    results = {}\n",
    "    for problem in problem_set[difficulty]:\n",
    "        results[problem] = get_problem_results(problem)\n",
    "    return results\n",
    "\n",
    "def get_problem_results(problem):\n",
    "    '''Input prompt, Output prompt, Token Usage, Output'''\n",
    "    return {\n",
    "        \"jac\": get_problem_result(problem, \"jac\"),\n",
    "        \"dspy\": get_problem_result(problem, \"dspy\"),\n",
    "    }\n",
    "\n",
    "def get_problem_result(problem, impl=\"jac\"):\n",
    "    _output = {\n",
    "        \"llm_requests\": [],\n",
    "        \"output\": \"\"\n",
    "    }\n",
    "    \n",
    "    file = f\"{RESULTS_DIR}/{problem}/{impl}/results.txt\"\n",
    "    with open(file) as f:\n",
    "        file_contents = f.read()\n",
    "    \n",
    "    while True:\n",
    "        input_prompt_pattern = r'Input Prompt:\\n(.*?)\\nOutput:'\n",
    "        input_prompt_match = re.search(input_prompt_pattern, file_contents, re.DOTALL)\n",
    "        if input_prompt_match:\n",
    "            input_prompt = input_prompt_match.group(1).strip()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        output_pattern = r'Output:\\n(.*?)\\n\\{' if impl == \"dspy\" else r'Output:\\n(.*?)\\nCompletionUsage'\n",
    "        output_match = re.search(output_pattern, file_contents, re.DOTALL)\n",
    "        if output_match:\n",
    "            output = output_match.group(1).strip()\n",
    "\n",
    "        file_contents = file_contents[output_match.end():]\n",
    "        if impl == \"dspy\":\n",
    "            slide = 0\n",
    "            token_pattern = r\"'completion_tokens': (\\d+), 'prompt_tokens': (\\d+), 'total_tokens': (\\d+)}\\n\"\n",
    "        else:\n",
    "            slide = 1\n",
    "            token_pattern = r\"(completion_tokens=(\\d+), prompt_tokens=(\\d+), total_tokens=(\\d+))\"\n",
    "        token_match = re.search(token_pattern, file_contents)\n",
    "        if token_match:\n",
    "            completion_tokens = int(token_match.group(1+slide))\n",
    "            prompt_tokens = int(token_match.group(2+slide))\n",
    "            total_tokens = int(token_match.group(3+slide))\n",
    "\n",
    "        file_contents = file_contents[token_match.end():]\n",
    "\n",
    "        _output[\"llm_requests\"].append({\n",
    "            \"prompt\": input_prompt,\n",
    "            \"output\": output,\n",
    "            \"token_usage\": {\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"total_tokens\": total_tokens,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    _output[\"output\"] = file_contents.strip()\n",
    "    return _output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for difficulty in problem_set.keys():\n",
    "    _results = get_results(difficulty)\n",
    "    with open(f\"{RESULTS_DIR}/{difficulty}.json\", \"w\") as f:\n",
    "        json.dump(_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Token Usage and Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(difficulty):\n",
    "    with open(f\"{RESULTS_DIR}/{difficulty}.json\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "_results = load_results('EASY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_costs = {\n",
    "    \"completion_tokens\": 15/1e6,\n",
    "    \"prompt_tokens\":5/1e6,\n",
    "}\n",
    "\n",
    "def get_results_df(results):\n",
    "    rows = []\n",
    "    for problem, result in results.items():\n",
    "        jac = result[\"jac\"]\n",
    "        dspy = result[\"dspy\"]\n",
    "        rows.append({\n",
    "            \"problem\": problem,\n",
    "            \"jac_completion_tokens\": sum([llm[\"token_usage\"][\"completion_tokens\"] for llm in jac[\"llm_requests\"]]),\n",
    "            \"jac_prompt_tokens\": sum([llm[\"token_usage\"][\"prompt_tokens\"] for llm in jac[\"llm_requests\"]]),\n",
    "            \"jac_total_tokens\": sum([llm[\"token_usage\"][\"total_tokens\"] for llm in jac[\"llm_requests\"]]),\n",
    "            \"dspy_completion_tokens\": sum([llm[\"token_usage\"][\"completion_tokens\"] for llm in dspy[\"llm_requests\"]]),\n",
    "            \"dspy_prompt_tokens\": sum([llm[\"token_usage\"][\"prompt_tokens\"] for llm in dspy[\"llm_requests\"]]),\n",
    "            \"dspy_total_tokens\": sum([llm[\"token_usage\"][\"total_tokens\"] for llm in dspy[\"llm_requests\"]]),\n",
    "            \"jac_cost\": sum([llm[\"token_usage\"][\"completion_tokens\"]*llm_costs[\"completion_tokens\"] + llm[\"token_usage\"][\"prompt_tokens\"]*llm_costs[\"prompt_tokens\"] for llm in jac[\"llm_requests\"]]),\n",
    "            \"dspy_cost\": sum([llm[\"token_usage\"][\"completion_tokens\"]*llm_costs[\"completion_tokens\"] + llm[\"token_usage\"][\"prompt_tokens\"]*llm_costs[\"prompt_tokens\"] for llm in dspy[\"llm_requests\"]]),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@kugesan will add graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Taken Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "\n",
    "p_dspy = pstats.Stats('/Users/chandralegend/Desktop/Jaseci/mtllm-evaluation/results/joke_gen/dspy/profile.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  6 01:12:09 2024    /Users/chandralegend/Desktop/Jaseci/mtllm-evaluation/results/joke_gen/dspy/profile.prof\n",
      "\n",
      "         71020 function calls (66822 primitive calls) in 2.801 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1130 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    300/1    0.000    0.000    2.801    2.801 /opt/conda/envs/mtllm-eval/lib/python3.12/importlib/__init__.py:73(import_module)\n",
      "    301/1    0.000    0.000    2.801    2.801 <frozen importlib._bootstrap>:1375(_gcd_import)\n",
      "    301/1    0.000    0.000    2.801    2.801 <frozen importlib._bootstrap>:1349(_find_and_load)\n",
      "      2/1    0.000    0.000    2.801    2.801 <frozen importlib._bootstrap>:1304(_find_and_load_unlocked)\n",
      "        2    0.000    0.000    2.801    1.400 <frozen importlib._bootstrap>:911(_load_unlocked)\n",
      "        1    0.000    0.000    2.800    2.800 <frozen importlib._bootstrap_external>:989(exec_module)\n",
      "      5/4    0.000    0.000    2.798    0.699 <frozen importlib._bootstrap>:480(_call_with_frames_removed)\n",
      "        1    0.000    0.000    2.797    2.797 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    2.797    2.797 /workspaces/mtllm-evaluation/easy/joke_gen/jokes_dspy.py:1(<module>)\n",
      "        3    0.000    0.000    2.786    0.929 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dspy/primitives/program.py:25(__call__)\n",
      "        3    0.000    0.000    2.786    0.929 /workspaces/mtllm-evaluation/easy/joke_gen/jokes_dspy.py:30(forward)\n",
      "        3    0.000    0.000    2.786    0.929 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dspy/predict/predict.py:48(__call__)\n",
      "        3    0.000    0.000    2.786    0.929 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dspy/predict/predict.py:51(forward)\n",
      "        3    0.000    0.000    2.785    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dsp/primitives/predict.py:64(do_generate)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dsp/modules/gpt3.py:157(__call__)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/backoff/_sync.py:85(retry)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dsp/modules/gpt3.py:139(request)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dsp/modules/gpt3.py:115(basic_request)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dsp/modules/gpt3.py:269(chat_request)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dsp/modules/cache_utils.py:14(wrapper)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dsp/modules/gpt3.py:263(v1_cached_gpt3_turbo_request_v2_wrapped)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/joblib/memory.py:654(__call__)\n",
      "        3    0.000    0.000    2.784    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/joblib/memory.py:500(_cached_call)\n",
      "        3    0.000    0.000    2.783    0.928 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/joblib/memory.py:832(call)\n",
      "        3    0.000    0.000    2.777    0.926 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/dsp/modules/gpt3.py:256(v1_cached_gpt3_turbo_request_v2)\n",
      "        3    0.000    0.000    2.776    0.925 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/_utils/_utils.py:243(wrapper)\n",
      "        3    0.000    0.000    2.776    0.925 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/resources/chat/completions.py:558(create)\n",
      "        3    0.000    0.000    2.706    0.902 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/_base_client.py:1226(post)\n",
      "        3    0.000    0.000    2.706    0.902 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/_base_client.py:912(request)\n",
      "        3    0.000    0.000    2.706    0.902 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/_base_client.py:929(_request)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x128da4b00>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_dspy.sort_stats('cumulative').print_stats(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_jac = pstats.Stats('/Users/chandralegend/Desktop/Jaseci/mtllm-evaluation/results/joke_gen/jac/profile.prof')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun  6 01:12:09 2024    /Users/chandralegend/Desktop/Jaseci/mtllm-evaluation/results/joke_gen/jac/profile.prof\n",
      "\n",
      "         95852 function calls (90266 primitive calls) in 1.300 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1206 to 30 due to restriction <30>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      2/1    0.000    0.000    1.300    1.300 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/plugin/feature.py:74(jac_import)\n",
      "      3/1    0.000    0.000    1.300    1.300 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/vendor/pluggy/_hooks.py:475(__call__)\n",
      "      3/1    0.000    0.000    1.300    1.300 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/vendor/pluggy/_manager.py:106(_hookexec)\n",
      "      3/1    0.000    0.000    1.300    1.300 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/vendor/pluggy/_callers.py:28(_multicall)\n",
      "      2/1    0.000    0.000    1.300    1.300 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/plugin/default.py:176(jac_import)\n",
      "      2/1    0.000    0.000    1.300    1.300 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/core/importer.py:17(jac_importer)\n",
      "        1    0.000    0.000    1.262    1.262 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    1.262    1.262 /workspaces/mtllm-evaluation/easy/joke_gen/jokes.jac:1(<module>)\n",
      "        1    0.000    0.000    1.240    1.240 /workspaces/mtllm-evaluation/easy/joke_gen/jokes.jac:10(tell_a_joke)\n",
      "        1    0.000    0.000    1.240    1.240 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/plugin/feature.py:258(with_llm)\n",
      "        1    0.000    0.000    1.240    1.240 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/plugin/default.py:567(with_llm)\n",
      "        1    0.000    0.000    1.240    1.240 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/core/aott.py:15(aott_raise)\n",
      "        1    0.000    0.000    1.240    1.240 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/core/llms/base.py:119(__call__)\n",
      "        1    0.000    0.000    1.240    1.240 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/jaclang/core/llms/openai.py:52(__infer__)\n",
      "        1    0.000    0.000    1.240    1.240 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/_utils/_utils.py:243(wrapper)\n",
      "        1    0.000    0.000    1.240    1.240 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/resources/chat/completions.py:558(create)\n",
      "        1    0.000    0.000    1.235    1.235 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/_base_client.py:1226(post)\n",
      "        1    0.000    0.000    1.235    1.235 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/_base_client.py:912(request)\n",
      "        1    0.000    0.000    1.235    1.235 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/openai/_base_client.py:929(_request)\n",
      "        1    0.000    0.000    1.233    1.233 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpx/_client.py:881(send)\n",
      "        1    0.000    0.000    1.233    1.233 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpx/_client.py:930(_send_handling_auth)\n",
      "        1    0.000    0.000    1.233    1.233 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpx/_client.py:964(_send_handling_redirects)\n",
      "        1    0.000    0.000    1.233    1.233 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpx/_client.py:1001(_send_single_request)\n",
      "        1    0.000    0.000    1.232    1.232 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpx/_transports/default.py:214(handle_request)\n",
      "        1    0.000    0.000    1.232    1.232 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:159(handle_request)\n",
      "        1    0.000    0.000    1.231    1.231 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpcore/_sync/connection.py:67(handle_request)\n",
      "        1    0.000    0.000    1.217    1.217 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpcore/_sync/http11.py:72(handle_request)\n",
      "        4    0.000    0.000    1.216    0.304 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpcore/_sync/http11.py:216(_receive_event)\n",
      "        1    0.000    0.000    1.216    1.216 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpcore/_sync/http11.py:179(_receive_response_headers)\n",
      "        3    0.000    0.000    1.216    0.405 /opt/conda/envs/mtllm-eval/lib/python3.12/site-packages/httpcore/_backends/sync.py:122(read)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x128d92d80>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_jac.sort_stats('cumulative').print_stats(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtllm-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
