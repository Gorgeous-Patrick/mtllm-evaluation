model_used: gpt-4o
dspy_max_token = 1024 on level generation, rest kept the default. default is set to 1024
dspy_temperature = 0.0 (default)

* jac implementations were not ran. as it similar to runs/no_semstr_results_24_6_18

Experimentation
- Checking whether even giving additional room for generation to dspy when using the chat models improves the results

Conclusion
- Additional room for generation didnt improved the result of dspy. It seems dspy is able to work better with chat models
when typed outputs are expected. when there is no specific mention of typed (json schema), dspy seems to rephrase unnecessarily.
- It is not advisable to use higher max token limit on dspy when using chat models. see the graphs